{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net til segmentering av EO satellittbilder\n",
    "\n",
    "Krever: torch, torchvision, torchgeo, geopandas, cv2, numpy, matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klargjøring av datasett\n",
    "\n",
    "En god måte å starte å eksperimentere med maskinlæring på først teste med noen eksisterende datasett. I denne notebooken skal vi bruke [Landcover AI](https://paperswithcode.com/dataset/landcover-ai) som inneholder annoterte EO bilder med segmentering av 4 kategorier: Skog, vei, vann og bygninger. Datasettet kan laststes ned fra deres deres nettside direkte, men finnes også som ett av flere datasett i *torchgeo* biblioteket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importere torchgeo\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchgeo.datasets import LandCoverAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laste ned og opprette trenings og test sett\n",
    "\n",
    "landcover_train = LandCoverAI(\"data/landcover\",split=\"train\",download=True)\n",
    "landcover_test = LandCoverAI(\"data/landcover\",split=\"test\",download=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta en titt i mappen for å se hvordan dataen har lagt seg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lage dataloaders\n",
    "batch_size = 8\n",
    "trainloader = torch.utils.data.DataLoader(landcover_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(landcover_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = landcover_train[2000]\n",
    "\n",
    "plt.imshow(image[\"image\"].permute(1,2,0).int())\n",
    "plt.matshow(image[\"mask\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sette opp U-Net\n",
    "\n",
    "Vi sskal sette opp et klassisk U-Net, bestående av en encoder og en decoder.\n",
    "\n",
    "![](../media/Unet.png)\n",
    "\n",
    "For å gjøre det mer oversiktelig deler vi det opp i flere små biter. Først koder vi en standard konvolusjonsblokk. Så bruker vi blokkene til å lage dekoderen. Også lager vi en dekoder og setter det sammen til det fullstendige nettverket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn  as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sette opp et en blokk for dobbelt 3x3 konvolusjonslag med ReLU aktivering\n",
    "class Block(nn.Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(inChannels, outChannels, 3,padding=1)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.conv2 = nn.Conv2d(outChannels, outChannels, 3, padding=1)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,channels=[3,32,64,128]):\n",
    "        super().__init__()\n",
    "        # Encoder funksjoner\n",
    "        self.channels = channels\n",
    "        self.blocks = nn.ModuleList([Block(channels[i],channels[i+1]) for i in range(len(channels)-1)])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward for encoder\n",
    "        block_outputs = []\n",
    "        for i in range(len(self.blocks)-1):\n",
    "            x = self.blocks[i](x)\n",
    "            block_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.blocks[-1](x)\n",
    "        block_outputs.append(x)\n",
    "        return block_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,channels=[128,64,32]):\n",
    "        super().__init__()\n",
    "        # Decoder funksjoner\n",
    "        self.channels = channels\n",
    "        self.blocks = nn.ModuleList([Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)])  \n",
    "        self.upConvs = nn.ModuleList([nn.ConvTranspose2d(channels[i], channels[i + 1], 2, 2) for i in range(len(channels) - 1)]) \n",
    "    def forward(self,x,block_outputs):\n",
    "        x = self.upConvs[0](x)\n",
    "        # forward for decoder\n",
    "        for i in range(1,len(self.channels)-1):\n",
    "            x = self.upConvs[i](x)\n",
    "            x = torch.cat([x,block_outputs[i]],dim=1)\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        return x\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\tdef __init__(self, encChannels=[3, 32, 64, 128],\n",
    "\t\tdecChannels=[128,64, 32],\n",
    "\t\tnbClasses=5,\n",
    "\t\toutSize=(512,512)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize encoder\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\t# initialize decoder\n",
    "\t\tself.head = nn.Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\t\tself.outSize = outSize\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0], encFeatures[::-1][1:])\n",
    "\n",
    "\t\tmap = self.head(decFeatures)\n",
    "\n",
    "\t\tmap = F.interpolate(map,(512,512))\n",
    "\t\treturn map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = torch.zeros(4,3,512,512)\n",
    "\n",
    "out = UNet()(test_im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchinfo import summary\n",
    "\n",
    "#model = UNet()\n",
    "\n",
    "#summary(model,(4,3,512,512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sette opp en treningsloop\n",
    "\n",
    "Vi er nå klare for å trene nettverket vårt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet()\n",
    "net.train()\n",
    "if torch.cuda.is_available():\n",
    "    net.to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definer tapsfunksjon og optimeringsalgoritme\n",
    "import torch.optim as optim\n",
    "\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lage en accuracy test\n",
    "import numpy as np\n",
    "def test_accuracy(batch_lim=None):\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(testloader):\n",
    "            images = batch[\"image\"]/255\n",
    "            labels = batch[\"mask\"]\n",
    "            if torch.cuda.is_available():\n",
    "                out = net(images.to(\"cuda\")).to(\"cpu\")\n",
    "            else:\n",
    "                out = net(images)\n",
    "            _, prediction = torch.max(torch.softmax(out,dim=1),dim=1)\n",
    "            pixel_accuracy = torch.sum(torch.eq(prediction,labels))/prediction.numel()\n",
    "            acc.append(pixel_accuracy)\n",
    "            if batch_lim is None:\n",
    "                pass\n",
    "            elif i > batch_lim:\n",
    "                break\n",
    "    return np.mean(np.array(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "accuracy_log = []\n",
    "current_best = 0.2\n",
    "for e in range(5):\n",
    "    # lage variabler for totalt tap\n",
    "    running_loss = 0\n",
    "    for i,batch in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[\"image\"]/255\n",
    "        labels = batch[\"mask\"]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "\n",
    "        out = net(images)\n",
    "        loss = lossfunc(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199 or i == 0:\n",
    "            if i == 0:\n",
    "                avg_loss = running_loss\n",
    "            else:\n",
    "                avg_loss = running_loss/(200)\n",
    "            print(f\"ep: {e+1}: batch: {(i+1)} Avg batch loss {avg_loss}\")\n",
    "            loss_log.append(avg_loss)\n",
    "            running_loss = 0\n",
    "        \n",
    "            accuracy = test_accuracy()\n",
    "            print(f\"Accuracy {accuracy}\")\n",
    "            accuracy_log.append(accuracy)\n",
    "            if accuracy > current_best:\n",
    "                torch.save(net.state_dict(),\"models/best_unet_X.pt\")\n",
    "                current_best = accuracy\n",
    "                print(\"new model saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vis frem loss - loggen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)\n",
    "plt.show()\n",
    "plt.plot(accuracy_log)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og inspiser noen resultater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def matshow(img):\n",
    "    npimg = img.numpy()[0]\n",
    "    plt.matshow(npimg,vmin=0,vmax=4)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def view_batch(batch):\n",
    "    image = batch[\"image\"]/255\n",
    "    labels = batch[\"mask\"]\n",
    "    if torch.cuda.is_available():\n",
    "        out = net(image.to(\"cuda\")).to(\"cpu\")\n",
    "    else:\n",
    "        out = net(image)\n",
    "        \n",
    "    _, prediction = torch.max(torch.softmax(out,dim=1),dim=1)\n",
    "    accuracy = torch.sum(torch.eq(prediction,labels))/prediction.numel()    \n",
    "\n",
    "    image = image/255\n",
    "    labels = labels.unsqueeze(1).repeat(1,3,1,1)\n",
    "    prediction = prediction.unsqueeze(1).repeat(1,3,1,1)\n",
    "\n",
    "\n",
    "    imshow(torchvision.utils.make_grid(image*255))\n",
    "    matshow(torchvision.utils.make_grid(labels))\n",
    "    matshow(torchvision.utils.make_grid(prediction))\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last inn den beste modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definer et nett\n",
    "net = UNet()\n",
    "if torch.cuda.is_available():\n",
    "    net = net.to(\"cuda\")\n",
    "\n",
    "#print(test_accuracy(10))\n",
    "\n",
    "# last inn beste modell\n",
    "if torch.cuda.is_available():\n",
    "    net.load_state_dict(torch.load(\"models/bestUnet.pt\",map_location=\"cuda\"))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(\"models/bestUnet.pt\",map_location=\"cpu\"))\n",
    "\n",
    "#print(test_accuracy(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(loop)\n",
    "\n",
    "view_batch(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hva om vi vil kjøre nettverket vårt på georeferert satellittdata?\n",
    "Vi kan ta en av GeoTiffene fra mappen *images*, og laste den inn som et `GeoImage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# velg nett\n",
    "net = UNet()\n",
    "if torch.cuda.is_available():\n",
    "    net = net.to(\"cuda\")\n",
    "    net.load_state_dict(torch.load(\"models/bestUnet.pt\",map_location=\"cuda\"))\n",
    "else:\n",
    "    net.load_state_dict(torch.load(\"models/bestUnet.pt\",map_location=\"cpu\"))\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legg til src i path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "src_path = Path(\".\").absolute().parent / Path('src')\n",
    "sys.path.append(src_path.__str__())\n",
    "\n",
    "from geoutils import GeoImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = \"data/landcover/images/N-33-60-D-d-1-2.tif\"\n",
    "\n",
    "image = GeoImage(fil)\n",
    "\n",
    "#finn høyde og bredde\n",
    "c,H,W = image.data.shape\n",
    "\n",
    "# lag en maske som er like stor som bildet\n",
    "mask_data = np.zeros((1,H,W))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vårt nettverk tar inn bilder av størrelse 512x512, vi må derfor gå gjennom det originale bildet og gjøre nettverket på såkalte \"tiles\" eller \"chips\" av den størrelsen. Etter hvert som vi regner ut segmenteringsmaskene, fyller vi dem inn i `mask_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,H-512,512):\n",
    "    for j in range(0,W-512,512):\n",
    "        local_image=image.data[:,i:i+512,j:j+512]/255\n",
    "        local_image = torch.from_numpy(local_image).unsqueeze(0).float()\n",
    "        if torch.cuda.is_available():\n",
    "            out = net(local_image.to(\"cuda\")).to(\"cpu\")\n",
    "        else:\n",
    "            out = net(local_image)\n",
    "        \n",
    "        _, prediction = torch.max(torch.softmax(out,dim=1),dim=1)\n",
    "        mask_data[0,i:i+512,j:j+512] = prediction\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå kan vi skrive ut masken vi har laget til koordinetene til input-bildet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoutils import CopyGeoTransform\n",
    "\n",
    "mask_image = CopyGeoTransform(image,mask_data,\"segmentert_maske.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
