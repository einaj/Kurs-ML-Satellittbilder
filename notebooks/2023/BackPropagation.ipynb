{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation: Regning og koding\n",
    "\n",
    "Backproagation er algoritmen som oppdaterer vektene i et feedforward nettverk, og er helt sentral del av læringsprosessen. Å ha en grunnleggend forståelse for denne prossesen er viktig for å skjønne hvordan moderne maskinlæring virker. I denne seksjonen skal vi gå litt i dybden på matematikken som ligger bak, både med regning og koding. For kode-delen tar denne notebooken i bruk biblioteket [Micrograd](https://github.com/karpathy/micrograd), som er utviklet av Andrej Karapthy.\n",
    "\n",
    "Vi vil holde oss til små eksempler, for at konseptet skal være lettere å forstå og regne på."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grunnprinsippet\n",
    "\n",
    "I sin enkleste form er prinsippet som følger: En input sendes gjennon nettverket, der det gjennomføres en serie med linære operasjoner. Disse linære operasjonene består av *vekter* $W$, og *bias* $b$, på formen $$y = W \\cdot x + b$$der $x$ er input og $y$ er output i hver operasjon. I et stort nettverk gjøres veldig mange av disse operasjonenen etter hverandre.\n",
    "\n",
    "![](../media/AI_basic_nevron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Til slutt får vi en endelig output, som vi gjerne kaller nettverkets prediksjon. Under veiledet (supervised) trening sammenliknes denne prediksjonen med en kjent sannhet. Forskjellen mellom prediksjonen og sannheten kalles *tapet* (loss) og betegnes ofte $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et veldig enkelt eksempel\n",
    "Gitt at vi har et nettverk bestående av ett nevron, med $W = -3$ og $b=10$, vi sender inn input $x = 2$. Hva blir outputen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2*-3 + 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi legger til ett nevron til, med $W = -2$ og $b=0$. Nettverket vårt er nå\n",
    "\n",
    "![](../media/simple_node.png)\n",
    "\n",
    "Hva er outputen nå?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan implementere nettverket vårt i en enkel python funksjon, og se hva slags output det vil gi for ulike verdier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettverk(x,w1,b1,w2,b2):\n",
    "    y1 = x*w1 + b1\n",
    "    y2 = y1*w2 +b2\n",
    "    return y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nettverk(2,-3,10,-2,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kort om optimering\n",
    "Under trening ønsker vi å finne de vektene som gir lavest tap, og dermed det mest presise nettverket. Dette gjøres ved å regne ut tapet, og justere vektene ett lite steg i den retningen som gjør tapet mindre. Denne metoden kalles *gradient descent*.\n",
    "\n",
    "![](../media/gradient_descent.jpg)\n",
    "\n",
    "Backpropagation handler om å regne ut nettopp hva denne justeringen skal være.\n",
    "\n",
    "### Hvordan påvirker vektene tapet?\n",
    "\n",
    "For å vite hva som er riktig justering av vektene, er vi avhengig av å vite hva sammenhengen mellom vektene og tapet er. Hvor mye vil en endring i vektene påvirke outputen?\n",
    "\n",
    "Outputen er en funksjon av alle vektene og biasen, så relasjonen mellom endringene der og endringene til tapet er gitt ved gradientene til hver av dem med hensyn på tapet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I vårt lille eksempel har vi to nevroner med vekter og bias, for enkelthetes skyld kan vi kalle dem $W_1, b_1, W_2$ og $b_2$. Vi setter også tapet i denne sammenhengen til å være $L = (y_2 - y_*)^2/2$ , der $y_2$ er outputen etter det andre laget, og $y_*$ er det vi ønsker at nettverket skal produsere.\n",
    "\n",
    "$$\\begin{align}\n",
    "y_1 = & W_1 \\cdot x + b_1 \\\\\n",
    "y_2 = & W_2 \\cdot y_1 + b_2 \\\\\n",
    "L = & \\frac{(y_2-y_*)^2}{2}\n",
    "\\end{align}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan først regne for nevron 2. Her må vi bruke kjerneregelen:\n",
    "\n",
    "<!--\n",
    "$$\\begin{align*}\n",
    "\\nabla _{y_2} &= \\frac{dL}{dy_2} = (y_2-y_*)\\\\\n",
    "\\nabla _{W_2} &= \\frac{dL}{dW_2} = \\frac{dL}{dy_2} \\cdot \\frac{dy_2}{dW_2} = \\nabla _{y_2} \\cdot y_1 \\\\\n",
    "\\nabla _{b_2} &= \\frac{dL}{db_2} = \\frac{dL}{dy_2} \\cdot \\frac{dy_2}{db_2} =  \\nabla _{y_2} \\cdot 1 \\\\\n",
    "\\end{align*}$$\n",
    "-->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og beveger oss bakover til nevron 1.\n",
    "\n",
    "<!--\n",
    "$$\\begin{align*}\n",
    "\\nabla _{y_1} &= \\frac{dL}{dy_1} = \\frac{dL}{dy_2} \\cdot \\frac{dy_2}{dy_1} = \\nabla _{y_2} \\cdot W_2\\\\\n",
    "\\nabla _{W_1} &= \\frac{dL}{dW_1} = \\frac{dL}{dy_2} \\cdot \\frac{dy_2}{dy_1} \\cdot \\frac{dy_1}{dW_1} = \\nabla _{y_1}\\cdot x \\\\\n",
    "\\nabla _{b_1} &= \\frac{dL}{db_1} = \\frac{dL}{dy_2} \\cdot \\frac{dy_2}{dy_1} \\cdot \\frac{dy_1}{db_1} = \\nabla _{y_1} \\cdot 1 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Her ser vi at $\\nabla _{y_2}$ nevron 2 blir brukt til å regne ut gradienten for nevron 1. Selv om dette er et lite eksempel vil denne oppførselen være lik bare i mye større skala for dypere nettverk. Gradientene blir først regnet ut bakerst i nettverket, og \"flyter\" bakover, der de påvirker gradientene til de tidligere nevronene. \n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave: Hva blir verdien til gradientene?\n",
    "Vi har at $W_1 = -3, b_1=10, W_2 = -2, b_2=0$, og vi sender inn input $x=2$. Gitt at fasiten var $y_* = -5$, hva blir gradientene i eksempelet? Regn ut for hånd.\n",
    "\n",
    "Vi kan verifisere svarene med micrograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legg til src i path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "src_path = Path(\".\").absolute().parent / Path('src')\n",
    "sys.path.append(src_path.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backprop_functions import Value, draw_dot\n",
    "# sett opp variabler\n",
    "x = Value(2,label=\"x\")\n",
    "w1 = Value(-3, label=\"w1\")\n",
    "b1 = Value(10, label=\"b1\")\n",
    "y1 = x*w1 + b1  ; y1.label = \"y1\"\n",
    "w2 = Value(-2, label=\"w2\")\n",
    "b2 = Value(0, label=\"b2\")\n",
    "y2 = y1*w2 + b2 ; y2.label=\"y2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sett opp tap\n",
    "y_ = -5\n",
    "L = 0.5*(y2-y_)**2 ; L.label = \"L\"\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gjør back propagation\n",
    "L.backward()\n",
    "draw_dot(L)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et (litt mindre) enkelt eksemel\n",
    "\n",
    "Vanligvis nat et nevron i et nettverk mer input fra mye mer enn ett nevron. Da har de gjerne en vekt per input. La oss se på ett litt mer komplekst eksempel, med 4 sett med vekter og bias.\n",
    "\n",
    "![](../media/double_node.png)\n",
    "\n",
    "Her er \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 = & W_1 \\cdot x + b_1 \\\\\n",
    "y_2 = & W_2 \\cdot x + b_2 \\\\\n",
    "y_3 = & W_{3_1} \\cdot y_1 + W_{3_2} \\cdot y_2 + b_3 \\\\\n",
    "L = & \\frac{(y_3-y_*)^2}{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi regner ut gradientene for det bakerste laget:\n",
    "<!--\n",
    "$$\\begin{align*}\n",
    "\\nabla _{y_3} &= \\frac{dL}{dy_3} = (y_3-y_*)\\\\\n",
    "\\nabla _{W_{3_1}} &= \\frac{dL}{dW_{3_1}} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dW_{3_1}} = \\nabla _{y_3} \\cdot y_1 \\\\\n",
    "\\nabla _{W_{3_2}} &= \\frac{dL}{dW_{3_2}} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dW_{3_2}} = \\nabla _{y_3} \\cdot y_2 \\\\\n",
    "\\nabla _{b_3} &= \\frac{dL}{db_3} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{db_3} =  \\nabla _{y_3} \\cdot 1 \\\\\n",
    "\\end{align*}$$ -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave: Hva blir utrykkene for gradientene i det siste laget?\n",
    "<!--\n",
    "Fasit:\n",
    "$$\\begin{align*}\n",
    "\\nabla _{y_2} &= \\frac{dL}{dy_2} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dy_2} = \\nabla _{y_3} \\cdot W_{3_2}\\\\\n",
    "\\nabla _{W_2} &= \\frac{dL}{dW_2} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dy_2} \\cdot \\frac{dy_2}{dW_2} = \\nabla _{y_2}\\cdot x\\\\\n",
    "\\nabla _{b_2} &= \\frac{dL}{db_2} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dy_2} \\cdot \\frac{dy_2}{db_2} = \\nabla _{y_2} \\cdot 1 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla _{y_1} &= \\frac{dL}{dy_1} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dy_1} = \\nabla _{y_3} \\cdot W_{3_1}\\\\\n",
    "\\nabla _{W_1} &= \\frac{dL}{dW_1} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dy_1} \\cdot \\frac{dy_2}{dW_1} = \\nabla _{y_1}\\cdot x\\\\\n",
    "\\nabla _{b_1} &= \\frac{dL}{db_1} = \\frac{dL}{dy_3} \\cdot \\frac{dy_3}{dy_1} \\cdot \\frac{dy_1}{db_1} = \\nabla _{y_1} \\cdot 1 \\\\\n",
    "\\end{align*}$$\n",
    "-->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave: Hva blir y?\n",
    "\n",
    "Gitt at vektene og biasene fra 1-3 er som vist i figuren og input er $x=2$, hva blir $y$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sjekk svar mot micrograd\n",
    "\n",
    "# sett opp variabler\n",
    "x = Value(2,label=\"x\")\n",
    "\n",
    "w1 = Value(-3, label=\"w1\")\n",
    "b1 = Value(10, label=\"b1\")\n",
    "y1 = x*w1 + b1  ; y1.label = \"y1\"\n",
    "\n",
    "w2 = Value(-2, label=\"w2\")\n",
    "b2 = Value(0, label=\"b2\")\n",
    "y2 = x*w2 + b2 ; y2.label=\"y2\"\n",
    "\n",
    "w31 = Value(4, label=\"w31\")\n",
    "w32 = Value(2, label=\"w32\")\n",
    "b3 = Value(-1, label=\"b3\")\n",
    "y3 = y1*w31 + y2*w32 + b3 ; y3.label=\"y3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y3)\n",
    "draw_dot(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppgave: Hva blir gradientene?\n",
    "\n",
    "Gi at riktig ønsket output er $y_* = 9$, hva blir gradientene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = 9\n",
    "L = 0.5*(y3-y_)**2\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppdatering av vekter\n",
    "\n",
    "Når vi har regnet ut alle gradientene, kan vi oppdatere vektene i nettverket. Dette gjøres ved\n",
    "\n",
    "$$ W_{\\text{new}} = W - \\lambda \\nabla _W $$\n",
    "\n",
    "Der $\\lambda$ er et lite tall, vi kaller læringsraten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backprop_functions import trace\n",
    "\n",
    "nodes,edges = trace(L)\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "for node in nodes:\n",
    "    node.data = node.data - lr*node.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a874a48c75579df9451fbd93930ec68fc6a5f646499e76a5df04ac1edb165619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
